---
title: "Ch3_sampling_the_imaginary"
author: "Samir Gadkari"
date: "4/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rethinking)
```

## 3.1 Sampling from a grid-approximate posterior

Let's sample from the posterior of the globe-tossing model using grid approximation to see the distribution of the posterior samples.

```{r}
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prob_prior <- rep(1, 1000)
prob_data <- dbinom(6, 9, prob = p_grid) # This is the likelihood.
posterior <- prob_prior * prob_data
posterior <- posterior / sum(posterior)

# Each sample is a value from 0 - 1 (since we're sampling p_grid,
# which is also from 0 - 1).
# The density of the samples shows the relative amounts of each number.
samples <- sample(p_grid,            # sample from the grid,
                  prob = posterior,  # using posterior probability
                  size = 1e4,
                  replace = TRUE)
plot(samples)
dens(samples)
```

## 3.2 Sampling to summarize the posterior
```{r}
str(posterior)
label_values = seq(0, 1000, 200)
plot(posterior,
     xaxt = "n",  # Don't show x axis
     xlab = "p_grid")
axis(1,           # 1 = below, 2 = left, 3 = above, 4 = right
     at = label_values,
     labels = as.character(round(label_values/1000, 2)))
```
### 3.2.1 Intervals of defined boundaries

Find the posterior probability where the proportion of water < 0.5.

```{r}
sum(posterior[p_grid < 0.5])
```
This is not easy to do when there are many parameters in the posterior distribution. So let's try getting this value by sampling the posterior. This process can be used with any number of parameters in the posterior.

```{r}
sum(samples < 0.5) / 1e4
```

The answer is almost the same as the actual posterior probability.

How much posterior probability lies between 0.5 and 0.75?
```{r}
sum((samples > 0.5) & (samples < 0.75))/ 1e4
```

### 3.2.2 Intervals of defined mass

Compatibility interval is usually called confidence interval, except we should not develop confidence in a standard procedure without thinking of what it means in real life. So we will call it the compatibility interval.

To find the lower 80 percent of the posterior distribution:
```{r}
quantile(samples, 0.8)
```
The middle 80 percent lies between 0.1 and 0.9 (the 10th and 90th percentile).
```{r}
quantile(samples, c(0.1, 0.9))
PI(samples, prob = 0.8)        # rethinking package function finds the middle
                               # x% range. PI = Percentile Interval.
```

Such intervals provide a description of the distribution as long as the distribution is approximately symmetrical. Let's try a non-symmetrical distribution for the globe toss example with 3 out of 3 events of water.

          experimental_prob
               |
               v
           likelihood <- p_grid
               |            |
               v            v
prior  ->  posterior ->  samples

```{r}
p_grid <- seq(0, 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(3, size = 3, prob = p_grid)
posterior <- prior * likelihood
posterior <- posterior / sum(posterior)

# These samples are sample probabilities
samples <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)

plot(p_grid, posterior)
```

```{r}
PI(samples, prob = 0.5)
```

But this range does not include the maximum probability (at 1). Instead, use HPDI function (High posterior density interval) to find this range.

```{r}
HPDI(samples, prob = 0.5)
```

The HPDI is sensitive to the number of samples you draw from the posterior (also called the simulation variance). If the choice of interval type makes a difference, just plot the entire posterior.

### 3.2.3 Point estimates

Point estimates of a posterior gives a single value to describe the posterior. Point estimates of a posterior discards information about it. So, you should not use it unless it is really necessary. To specify a point estimate, we must specify a loss function. Different loss functions nominate different point estimates.

To find the MAP (Maximum a Posteriori) for the 3 waters out of 3 tosses example:
```{r}
which.max(posterior)
p_grid[which.max(posterior)]
```

To use samples to find the MAP:
```{r}
# Find the mode from samples
chainmode(samples,     # values sampled from posterior
          adj = 0.01)
```

Mean and median:
```{r}
mean(samples)
median(samples)
```

So which do we choose? Mode, mean, median?
If you're using the absolute error as the criteria, then the median of the samples minimizes the loss function. If using the euclidean distance as the loss, the mean of the sample minimizes the loss function:
```{r}
median_loss_fn <- function(d) {
  sum(posterior * abs(d - p_grid))
}

loss <- sapply(p_grid, median_loss_fn)
p_grid[which.min(loss)]
```
```{r}
mean_loss_fn <- function(d) {
  sum(posterior * ((d - p_grid)^2))
}

loss <- sapply(p_grid, mean_loss_fn)
p_grid[which.min(loss)]
```

```{r}
plot(p_grid, posterior)
abline(v = median(samples), col = "blue")
abline(v = mean(samples), col = "red")

plot(p_grid, sapply(p_grid, median_loss_fn), 
     lty = 1, lwd = 2, 
     col = "blue",
     ylim = c(0, 0.2),
     ylab = "Absolute loss (blue) and Quadratic loss (red)")
lines(p_grid, sapply(p_grid, mean_loss_fn), lty = 1, lwd = 2, col = "red")
```

## 3.3 Sampling to simulate prediction

               experimental_prob
                      |
                      v
                  likelihood <- p_grid
                      |            |
                      v            v
       prior  ->  posterior ->  samples(model works correctly?
         |            | rbinom()        investigage model behavior)
         |            v                      \ rbinom()
         v         sample values               \
      samples(see what the model expects)        > sample values
         |
         | rbinom()
         v
    sample values
Generating implied observations from a model is useful for:

  * Model design: Sampling from the prior will tell us what the model expects. This is most useful when there are multiple parameters, so that the joint distribution needs to be sampled to be understood.
  * Model checking: After updating a model with data, generate sample observations from the model to:
    * check if the model works correctly
    * investigate model behavior
  * Software validation: To ensure model-fitting software is valid,
    * generate observations from the model
    * recover values of the parameters, and check against values of the original data
  * Research design: If you can simulate observations from your hypothesis, you can evaluate whether the research can be effective. This means it is not just power analysis, but much more.
  * Forecasting: Simulate new predictions for new cases and future observations. These forecasts are useful for
    * applied prediction
    * model criticism and revision
    
### 3.3.1 Dummy data

We will simulate the globe tossing model.

Likelihood is bi-directional:
  * Given a realized observation, the likelihood says how plausible it is
  * Given model parameters, the likelihood gives us a distribution of the possible observations
  
Binomial distribution: 

\begin{equation}
Pr(W|N,p) = \frac{N!}{(W! (N-W)!))} p^W (1-p)^{N-W}
\end{equation}

where W = number of times we hit water
      N = number of times globe is tossed
      p = probability of water
      1 - p = probability of land
      
If the globe was tossed 2 times, we would have hit water for 0, 1, 2 times.
```{r}
dbinom(0:2, size = 2, prob = 0.7)
```

With a 0.7 probability of hitting water, we would have landed on water
  * 0 times with a probability of 0.09
  * 1 times with a probability of 0.42, and
  * 2 times with a probability of 0.49
  
Using these probabilities, we can simulate observations. For example, if we want to simulate observing 1 water out of 2 tosses with a 0.7 probability of hitting water:
```{r}
rbinom(1, size = 2, prob = 0.7)
```

The value 1 we got indicates we hit water 1 time in 2 tosses.
Lets simulate more observations:

```{r}
rbinom(10, size = 2, prob = 0.7)
```
Each value above tells us how many times we hit water in 2 tosses.

Generating 100000 dummy values:
```{r}
dummy_w <- rbinom(1e5, size = 2, prob = 0.7)
table(dummy_w) / 1e5
```
These values are close to the dbinom's analytical output above.

Now let's simulate 9 tosses:
```{r}
dummy_w <- rbinom(1e5, size = 9, prob = 0.7)
simplehist(dummy_w, xlab = "dummy water counts")
```

In this book, the posterior distribution is deduced logically. Then samples can be drawn from it.

### 3.3.2 Model checking

  * Is the model fit good?
  * Is the model adequate?
  
#### 3.3.2.1 Did the software work?

Evaluate if the model fitting worked.

Model fit works like this:

  Prior  ->  Likelihood  -> Posterior  -> 

To see if the software worked, you can get the data working backwards from the predictions like this:

  Sampled Data  <-  Likelihood  <- Predictions
  Prior
  
If there is correspondence between the generated data, and the data used to fit the model, then it is a good fit.

#### 3.3.2.2 Is the model adequate?

Assess how the model fails to describe the data. This will enable model comprehension, revision, and improvement.

  Data  ->  Likelihood (uses p)  ->  Predictions (Posterior)
  Prior

The implied predictions of the model are uncertain in two ways:

  * Predicted observation uncertainity. Even though the probability (p) of water is known precisely, we don't know if the next toss will be water or land.
  * Uncertainity about the probability (p) of water. p is used to get the posterior distribution. This is why you get a posterior distribution, and not a single p value. Since p is uncertain, so is the posterior distribution, and anything derived from it.

We propagate the parameter uncertainity by averaging over the posterior density for p when computing the predictions. For each value of p, there is a distribution of outcomes. If you do a weighted average of all such distributions for all values of p, you get the Posterior Predictive Distribution. This distribution propagates uncertainity about parameter to uncertainity about prediction.

We can simulate predicted observations for a single value of p using rbinom:
```{r}
w <- rbinom(1e4, size = 9, prob = 0.6)
```

or we can use the samples generated from the posterior. These samples are generated using the likelihood and the prior, so they represent the full posterior probability distribution. They are also probabilities - not the number of water outcomes.

```{r}
p_grid <- seq(0, 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(6, size = 9, prob = p_grid)
posterior <- prior * likelihood
posterior <- posterior / sum(posterior)

samples <- sample(p_grid, size = 1e5, replace = TRUE, prob = posterior)
posterior[1:10]
samples[1:10]
```
We will now get the actual posterior values (1 for water, 0 for land):
```{r}
w <- rbinom(1e4, size = 9, prob = samples)
w[1:10]
simplehist(w)
```

We've so far seen the data as the model sees it. The model assumes each toss of the globe is independent of any other. But it is easy to create a dependency, since a normal person usually gets into a routine when carrying out a routine procedure. To look at aspects of our prediction, let's look at the data two different ways:

  * Length of the longest run of either water or land. This will provide a crude measure or correlation between tosses
  * The number of times the data switches from water to land or land to water

Let's use the sequence of nine tosses: W L W W W L W L W. So we have longest run of 3, and data switches of 6.

```{r}
num_switches <- function(x) {

  x_sign <- dplyr::case_when(x == 0 ~ -1, TRUE ~ 1)
  sum(diff(x_sign) != 0)
}

ws <- lapply(1:1e5, function(x) rbinom(9, 1, prob = samples))
switches <- sapply(ws, num_switches)

# rle calculates the lengths and values of runs of equal values in a vector
runs <- sapply(ws, function(x) max(rle(x)$lengths))
```
```{r}
simplehist(switches, xlab = "Number of switches", ylab = "Freq of switches")
abline(v = 6, col = "blue")
simplehist(runs, xlab = "Length of longest run", ylab = "Freq of runs")
abline(v = 3, col = "blue")
```
The blue line shows the number of switches and length of longest run in our sample. You can see that the number of switches is not close to the number of switches in the sample. This is consistent with a lack of independence between tosses of the globe. This means that each toss is providing less information about the true coverage of water on the globe. Even this bad model will still converge, but it will take a long time to do so.

The length of longest run matches the length of longest run of our sample.

A more common way of measuring extremes is to look at the tail values of data that are > 0.95 central probability mass. This leads to the p-value of 0.05. But this way measures the model just as it is defined, and so is a very weak form of model checking.