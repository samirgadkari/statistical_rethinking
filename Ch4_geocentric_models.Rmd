---
title: "Ch4_geocentric_models"
author: "Samir Gadkari"
date: "4/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
library(MASS)
```

We will look at linear regression as a bayesian model. Each parameter in the model will have a probability distribution.

## 4.1 Why normal distributions are normal

If you generate multiple samples from a random variable, the means of each sample form a normal distribution. The variable does not need to be normal.

The general form of this is:

> Take a number of random variables that have various kinds of distributions. Any one of these variables may or may not have a normal distribution. Adding up samples of these variables results in a normal distribution.

So the question is why?

There are many more ways of generating the mean than any other value after addition. There are a little less number of ways of generating the values next to the mean, and lesser number of ways going further from the mean. There are very few ways of generating values at the tails. This is the reason why adding random samples of various kinds of distributions results in a normal distribution.

The normal distribution is part of the exponential family of distributions. This family includes normal, exponential, Poisson, etc.

Simulate 16 steps of +1 or -1 from your current position. Sum up those values and plot to see the resulting distribution. Try squaring or taking the sine of those values before summing them up. All results lead to a normal or almost normal distribution.
```{r}
pos <- replicate(1000, # number of times to run expression
                 sum(runif(16, -1, 1)))
hist(pos, ylab = "Freq of Sum(random)")

pos <- replicate(1000, # number of times to run expression
                 sum(runif(16, -1, 1)^2))
hist(pos, ylab = "Freq of Sum(random^2)")

pos <- replicate(1000, # number of times to run expression
                 sum(sin(runif(16, -1, 1))))
hist(pos, ylab = "Freq of Sum(sin(random))")
```

Multiplying small numbers is the same as addition. Ex: 1.1 x 1.1 = `r 1.1 * 1.1`. Adding the increases gives us: (1 + 0.1) * (1 + 0.1) = 1 + 0.2 + 0.01 = 1.2. This is very close to the product. So small effects that multiply together are approximately additive.

```{r}
small <- replicate(1e4, prod(1 + runif(12, 0, 0.01)))
hist(small, ylab = "Freq of Product(small numbers)")

big <- replicate(1e4, prod(1 + runif(12, 0, 0.5)))
hist(big, ylab = "Freq of Product(bigger numbers)")
hist(log(big), ylab = "Freq of Log(Product(bigger numbers))")
```

Notice that even if the histogram of the product of big numbers is not normal, the histogram of the log of the product of big numbers is normal.

Since many natural processes add together fluctuations, the measurements of these processes are Gaussian. This is why Gaussian distributions are used for science. Another reason to use them is because we may only know that a measure has finite variance. The Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising, and least informative assumption to make. Thus it is the most consistent with our golem's assumptions.

Since those natural processes shed all underlying information about their processes when adding fluctuations, they cannot identify the underlying process.

There are many types of processes in nature that are not additive, thus not creating Gaussian distributions.

Many processes in nature can be represented using the Exponential family of distributions (exponential, gamma, Poisson, etc.).

The Gaussian distribution has very thin tails. Many processes have heavy tails, meaning that a high probability of extreme events exists with them. ex. financial time series can look Gaussian in the short term, but over medium/long term can result in extreme shocks. You should not use Gaussian models for such time series.

Discrete distributions like the Binomial are called Probability Mass functions.
Continuous distributions like the Gaussian are called Probability Density functions. For both, the area under the curve sums to 1. For Probability Density functions can have likelihoods > 1, but Probability Mass functions cannot. This is because Probability density is the rate of change in cumulative probability. So where cumulative probability is changing rapidly, the density can exceed 1.

## 4.3 Gaussian model of height

Our Bayesian machine considers many possible Gaussian distributions, and finds the one matching the given data and posterior plausibility. Note that we're finding the correct model by matching against the entire posterior plausibility - not just any point within it.

Let's load the Howell data about the Kalahari foraging people from the 1960s.
```{r}
data(Howell1)
d <- Howell1
```
```{r}
str(d)     # structure of the dataframe
precis(d)  # summarize the data
```

Select only people 18 and older:
```{r}
d2 <- d[d$age >= 18, ]
dens(d2$height)
```
Adult heights from a single population are almost always approximately normal. This one also looks close to normal.

Modeling by looking at the data is a really bad idea. If the data is a mixture of different Gaussians, you won't be able to tell the underlying normality by looking at the data. Also, the empirical distribution need not be normal for us to use a normal distribution in our model. Our model is the small-world, golem view of the world, after all.

|    h ~ Normal($\mu$, $\sigma$)  [likelihood, prior for heights]
|    $\mu$ ~ Normal(178, 20)      [$\mu$ prior]
|    $\sigma$ ~ Uniform(0, 50)    [$\sigma$ prior]

In this normal model, the $h_i$ are independent, identically distributed (IID). But we know that heights of girl siblings in a family are not independent, although heights of girls in a population are. Also, heights of each generation of, say males, in a family are also not independent. But our IID assumption here is the golem's mental (epistemological) view, not the real-world (ontological) view. We're assuming ignorance of the dependence of heights, and taking the conservative IID assumption.

Also, De Finetti's theorem says that if you can reorder values of a distribution, you can approximate these as mixtures of IID distributions.

To get a sense of the assumptions you're making, you should always plot your priors:
```{r}
sd = 20
curve(dnorm(x, 178, sd), from = 100, to = 250)
abline(v = 178, col = "blue")
abline(v = 178 - sd, lty = 2)
abline(v = 178 + sd, lty = 2)
abline(v = 178 - 2 * sd, lty = 3)
abline(v = 178 + 2 * sd, lty = 3)
abline(v = 178 - 3 * sd, lty = 4)
abline(v = 178 + 3 * sd, lty = 4)
```
```{r}
curve(dunif(x, 0, 50), from = -10, to = 60)
```

These plots make sense for height. Usually, average height is around 5'6", or `r round((12 * 5 + 6) * 2.5, 0)` cm, which is within $\pm 1 \sigma$ of the mean. We said sigma should be between 0 - 50. If the sigma is too large, the left hand side will go negative - which is not right for height. Sigma may not be 50 based on the data (95% of people lie between $\pm 100$ or $2 \sigma$), but we want to be conservative, and get a maximum value. It also makes sense to have lower bound of sigma as 0 since it is the standard deviation.

h, $\mu$ and $\sigma$ imply a joint posterior distribution of individual heights. To look at heights, you can sample from this prior:
```{r}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```
### 4.3.5 Finding the posterior distribution with quap

quap = calculates the quadratic approximation by:

  * using the model definition, then
  * defining the posterior at each combination of parameter values, then
  * climbing the posterior to find it's peak (it's MAP), then
  * estimating the quadratic curvature at the MAP to produce and approximation of the posterior.

```{r}
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

m4.1 <- quap(flist, data = d2)
precis(m4.1)
```
These numbers are the Gaussian approximations for each parameter's marginal distribution. This means, plausibility of each value of mu, after averaging over all sigma is given by a Gaussian distribution with mean 154.61 and standard deviation 0.41. The 5.5% and 94.5% values are the 89% compatibility intervals. If you want another interval, just put it into the precis function:
```{r}
precis(m4.1, prob=0.95)
```

You can provide quap a list of starting parameters to find the MAP value using:
```{r}
start <- list(mu = mean(d2$height), 
              sigma = sd(d2$height))
m4.1 <- quap(flist, data = d2, start = start)
```
When specifying formulas, use alist() so the code is not executed immediately (lazy execution for alist). When specifying start values, use list since it will be immediately executed to find the mean(d2$height), for example.

Let's change standard deviation for mu, and see what happens:
```{r}
m4.2 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),  # Changed sd from 20 to 0,1
    sigma ~ dunif(0, 50)
  ), data = d2
)
precis(m4.2)
```

Notice that the mean for mu is concentrated around 178. This is because we specified sigma for mu = 0.1. Also, notice that the mean for sigma has changed significantly, and so has the standard deviation. Since we restricted the golem to a small value of sd for mu, it took that and changed mean for sigma based on what it found for mu. The values for sigma changed although we did not change anything for sigma, because values for sigma were conditional on the values for mu.

### 4.3.6 Sampling from a quap

Since we have two parameters, we have a multi-dimensional ($\mu and \sigma) Gaussian distribution. When we sample from it, we will get a vector (instead of a single number).

The quadratic approximation calculates sd for all parameters, and also covariances among pairs of parameters. Just like the mean and sd are enough to describe a 1-D Gaussian, a list of means and the Variance-Covariance matrix are enough to describe the multi-dimensional Gaussian.

```{r}
vcov(m4.1)
```

Let's factor the vcov matrix into variances and correlations.
```{r}
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
```

The correlation between mu and sigma is very low. This is how it is for most of the simple models. This will become important when we add a regression line as a model parameter.

To sample the posterior, use extract.samples. These samples preserve the covariance between mu and sigma.
```{r}
post <- extract.samples(m4.1, n = 1e4)
head(post)
precis(post)
plot(post)
dens(post)
```

Sometimes, you want to simulate multi-variate Gaussian processes. In this case, you won't be able to use extract.samples, since it works on models only. Instead, use mvrnorm (multi-variate normal deviates):
```{r}
post <- mvrnorm(n = 1e4, mu = coef(m4.1), Sigma = vcov(m4.1))
```

We used the m4.1 model here for convinience. To simulate multi-variate Gaussian processes, we would need to define the mu vector and the Sigma martix.

## 4.4 Linear prediction

Let's look at the Kalahari foragers height against weight.
```{r}
plot(d2$height ~ d2$weight)
```

There is some relation. Let's model it using a Linear Regression, since it looks like it would work in this case.

The Probabilistic Linear Model estimates the parameters needed to draw the line/curve through the data. For all parameters, it produces a ranking of the plausibility of each value of that parameter.

|           $h_i$ ~ Normal($\mu_i$, $\sigma$)
|           $\mu_i$ = $\alpha + \beta_i (x_i - \bar x)$
|           $\alpha$ ~ Normal(178, 20)
|           $\beta$ ~ Normal(0, 10)
|           $\sigma$ ~ Uniform(0, 50)

#### 4.4.1.1 Probability of the data
The first line of the model shows the probability of the observed height. The little i on $\mu$ indicates that the mean depends on the row of data.

#### 4.4.1.2 Linear model
In the second line, the linear model, we used an = instead of a $\tilde$ because the relation on the RHS is deterministic.

$\alpha$ and $\beta$ are parameters we invented for manipulating $\mu$ across the data. Anytime we need to understand something about a model, we put in a parameter for it. Here we're asking:

  * What is the expected height when $x_i = \bar x$?
  * What is the change in expected height when $x_i$ changes by 1 unit?

These questions should be asked with respect to the observable variables.

You should choose a model based on substantive theory. Such models perform much better than any other model. In this case a Linear Model was the best, but we could have written an exponential model there, or any other equation there, as long as it matched the theory.

You can look at the units of your model to determine the units for $\alpha$ and $\beta$.

#### 4.4.1.3 Priors

Let's simulate heights from the model, using just the priors. This will tell us what the priors imply.
```{r}
N <- 100

prior_predictive_sim <- function(b) {
  set.seed(1795)
  a <- rnorm(N, 178, 20)

  plot(NULL, xlim = range(d2$weight), ylim = c(-100, 400),
       xlab = "weight", ylab = "height")
  abline(h = 0, lty = 2)               # No one is below 0 cm tall
  abline(h = 272, lty = 1, lwd = 0.5)  # Tallest person (Wadlow) line
  
  mtext("b ~ dnorm(0, 10)")
  xbar <- mean(d2$weight)
  for (i in 1:N)
    curve(a[i] + b[i] * (x - xbar),
          from = min(d2$weight), to = max(d2$weight),
          add = TRUE,  # add to existing plot
          col = col.alpha("black", 0.2)) # calculates transparent color values
}

prior_predictive_sim(rnorm(N, 0, 10))
```

Without looking at the data, we know that height increases with weight. So instead of $\beta$ being normal, we can define it as Log-normal. So the $log(\beta)$ is normal, which means $\beta$ is positive, since $e^{any number}$ is positive.

|           $\beta$ ~ Log-Normal(0, 1)

Let's see what this looks like:
```{r}
b <- rlnorm(N, 0, 1)
dens(b, xlim = c(0, 5),
     adj = 0.1)  # adj is the width of the density kernel
```

```{r}
prior_predictive_sim(rlnorm(N, 0, 1)) # rlnorm gives the log norm deviates
```

This is a much better prior. We have a lot of data, so we could have ignored this step, and continued with the earlier definition of $\beta$. The reasons to fuss about priors is:

  * There are many analyses where no amount of data makes the prior irrelevant
  * Thinking about priors helps us develop better models
  
There is no uniquely correct prior. By doing prior-predictive simulations, we're exploring the consequences of beginning with different information. By varying priors, we can see how our knowledge affects the analysis. When we do this exploration, we should not look at the data.

### 4.4.2 Finding the posterior distribution

So our new model is:

|           $h_i$ ~ Normal($\mu_i$, $\sigma$)
|           $\mu_i$ = $\alpha + \beta_i (x_i - \bar x)$
|           $\alpha$ ~ Normal(178, 20)
|           $\beta$ ~ Normal(0, 10)
|           $\sigma$ ~ Uniform(0, 50)

Let's fit the model:
```{r}
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18, ]

xbar <- mean(d2$weight)
m4.3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
```
When you pull samples from the posterior, the distribution of those samples approximates the posterior's distribution.

Another way of coding log-normal:
```{r}
m4.3b <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + exp(log_b) * (weight - xbar),
    a ~ dnorm(178, 20),
    log_b ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ), data = d2
)
```

Thm4.3b is will make the same predictions as m4.3. But instead of $\beta$ in the posterior, you will get $log(\beta)$.

### 4.4.3 Interpreting the posterior distribution

Once you get the posterior distribution, you have to make sense of it. You can either read tables to do that, or plot simulations. Most models are hard to understand from tables alone. So we will plot posterior distributions and predictions. This will allow you to:

  * see if the model fit worked
  * see the absolute magnitude of the relationship between outcome and predictor
  * find the uncertainity surrounding an average relationship
  * find the uncertainity surrounding the implied predictions of the model, as these are distinct from parameter uncertainity
  
Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model. These are small-world numbers, but disagreements about them leads to model revision, which is a good thing.

#### 4.4.3.1 Tables of marginal distributions

Let's inspect the marginal posterior distribution:
```{r}
precis(m4.3)
```
The 0.9 mean value of b (the slope) means a person 1kg heavier will be 0.9cm taller. 89% probability is within 0.84 and 0.97. This means slopes close to 0 or much higher than 1 are incompatible with this data. Just because we fitted a straight line to it, does not mean the relation between height and weight is linear. It just means that if you're committed to a line, then lines with a slope around 0.9 are the plausible ones.

```{r}
round(vcov(m4.3), 3)
```
No covariance among the parameters.
```{r}
pairs(m4.3)
```
The density is shown in the diagonal. The correlation is shown in the lower plots. The blue plots at the top are the scatterplots between variables.

#### 4.4.3.2 Plotting posterior inference against the data

Much better than tables is to plot posterior inference against the data. It helps to:

  * interpret the posterior
  * check on model assumptions
  
When the predictions aren't close to model observations or patterns, then you can look at if the model is not fit correctly or is badly specified.

Let's start with superimposing posterior mean values over the height and weight data.

```{r}
plot(height ~ weight, data = d2, col=rangi2)
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```

#### 4.4.3.3 Adding uncertainity around the mean

Every combination of $\alpha$ and $\beta$ is assigned a posterior probability. So let's visualize it.
```{r}
line_posterior_probability <- function(num_data, num_lines) {
  
  set.seed(8080)
  N <- num_data
  
  dN <- d2[1:N, ]
  mN <- quap(
    alist(
      height ~ dnorm(mu, sigma),
      mu <- a + b * (weight - mean(weight)),
      a ~ dnorm(178, 20),
      b ~ dlnorm(0, 1),
      sigma ~ dunif(0, 50)
    ), data = dN
  )
  
  post <- extract.samples(mN, n = num_lines)
  plot(dN$weight, dN$height, 
       xlim = range(d2$weight), ylim = range(d2$height),
       col = rangi2, xlab = "weight", ylab = "height")
  mtext(concat("N = ", N))
  
  for (i in 1:num_lines) {
    # You need to specify the variable x in curve() function.
    # This x is for the x-axis values that curve will fill in.
    # Curve does not know the position of these values in your
    # function, so you use x to specify the position.
    curve(post$a[i] + post$b[i] * (x - mean(dN$weight)),
          col = col.alpha("black", 0.3), add = TRUE)
  }
}

line_posterior_probability(10, 20)
line_posterior_probability(50, 20)
line_posterior_probability(150, 20)
line_posterior_probability(352, 20)
```
#### 4.4.3.4 Plotting regression intervals and contours

Let's see the uncertainity around the average regression line. Focus on weight value of 50kg. Let's make a list of 10,000 values of $\mu$ for a 50kg individual by sampling from the posterior.
```{r}
post <- extract.samples(m4.3)
mu_at_50 <- post$a + post$b * (50 - xbar)
dens(mu_at_50, col = rangi2, lwd = 2, xlab = "mu|weight = 50")
PI(mu_at_50, prob = 0.89)
```
Since the components of $\mu$ are Gaussian, $\mu$ is Gaussian as well. Adding Gaussians produces a Gaussian.

We need to repeat the above calculation for every weight value - not just 50kg. For this, use the link function. Basically, link just calculates the regression equation and produces the output.
```{r}
mu <- link(m4.3)
str(mu)
```
Each individual weight is used to produce a column of this matrix, mu. There are 1000 samples for each individual weight. What we really want is to calculate weight along the sequence of possible weights. So let's do this:
```{r}
weight.seq <- seq(25, 70, 1)
mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu)

plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))
# for (i in 1:100) 
#   points(weight.seq, mu[i, ],
#          pch = 16, col = col.alpha(rangi2, 0.1))

mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)

# lines joins the line segments given by the x and y coordinates
lines(weight.seq,  # x-coordinate
      mu.mean)     # y-coordinate

shade(mu.PI,  # density or formula object
      weight.seq) # x-axis value for plot
```

There are only 46 columns because there are only 46 values between 25 and 70.

The shading on the model above shows a very tight bound around the regression line. But you should note that this line and the bound is specific to the model. Even a bad model can have a tight bound.

The link function can be defined as:

> mu.link <- function(weight) post$a + post$b * (weight - xbar)

```{r}
post <- extract.samples(m4.3)
mu.link <- function(weight) post$a + post$b * (weight - xbar)
weight.seq <- seq(25, 70, by = 1)
mu <- sapply(weight.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
```

Knowing how each function works allows you to create your own functions for different use-cases.

#### 4.4.3.5 Prediction intervals

Now let's generate 89% prediction intervals for actual heights - not just for average height $\mu$.

|           $h_i$ ~ Normal($\mu_i$, $\sigma$)

So far we have sampled the posterior to visualize uncertainity in $\mu_i$. Now we also take into account $\sigma$, the spread around $\mu$. To do this:

  * sample $\sigma$ for each weight from the posterior distribution of $\mu$
  * sample from the Gaussian distribution with the correct mean $\mu$ for that weight, using the correct value of $\sigma$ for that weight

The sim tool can do this:
```{r}
sim.height <- sim(m4.3, data = list(weight = weight.seq),
                  n = 1e3) # specify number of samples to use
str(sim.height)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```
```{r}
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean) # lines joins the coords from 
                           # (x1,y1) to (x2,y1) to (x3,y3), etc.
shade(mu.HPDI, weight.seq)
shade(height.PI, weight.seq)
```

The wide shaded area represents the 89% of actual heights in the population at each weight.

The posterior distribution is a ranking of the relative plausibilities of every possible combination of parameter values. The distribution of simulated outcomes, like height, is instead a distribution that includes sampling variation from some process that generates Gaussian random variables.

```{r}
post <- extract.samples(m4.3)
weight.seq <- 25:70
sim.height <- sapply(weight.seq, 
                     function(weight)
                       rnorm(
                         n = nrow(post),
                         mean = post$a + post$b * (weight - xbar),
                         sd = post$sigma
                       ))
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

## 4.5 Curves from lines

Let's model the outcome as a curved function of a single predictor variable.

### 4.5.1 Polynomial regression
```{r}
data(Howell1)
d <- Howell1
str(d)
plot(d$height ~ d$weight)
```

This is a curved dataset because of the non-adult individuals included in it. Let's try to fit it with a parabolic model:

|         $\mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2$

The $\beta_2$ parameter measure the curvature of the relationship.

Standardize the predictor variable. This is especially helpful for polynomial models, because the square or the cube of a number can be really large. Standardizing should be your default behavior. The parabolic model is now:

|           $h_i$ ~ Normal($\mu_i$, $\sigma$)
|           $\mu_i$ = $\alpha + \beta_1 x_i + \beta_2 x_i^2)$
|           $\alpha$ ~ Normal(178, 20)
|           $\beta$ ~ Normal(0, 10)
|           $\sigma$ ~ Uniform(0, 50)

These polynomial parameters are 

  * very difficult to set realistic priors for
  * difficult to interpret
  
This is why we usually avoid them.

```{r}
d$weight_s <- (d$weight - mean(d$weight)) / sd(d$weight)
d$weight_s2 <- d$weight_s^2
m4.5 <- quap(
  alist(
      height ~ dnorm(mu, sigma),
      
      # This relationship uses standardized data
      # weight_s and weight_s2.
      mu <- a + b1 * weight_s + b2 * weight_s2,
      
      # The relationships below use un-standardized data
      a ~ dnorm(178, 20),
      b1 ~ dlnorm(0, 1),
      b2 ~ dnorm(0, 1), # need not be log-normal. -ve b2 is possible.
      sigma ~ dunif(0, 50)
    ), data = d
  )
precis(m4.5)
```
Since the relationship between the outcome height and the predictor weight depends on b1 and b2, it's not easy to read the relationship from the table of coefficients. Let's plot these model fits to understand them:

```{r}
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight_s = weight.seq, weight_s2 = weight.seq^2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.5, data = pred_dat)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)

plot(height ~ weight_s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

Let's use a higher order polynomial to see if the match is better:

|           $h_i$ ~ Normal($\mu_i$, $\sigma$)
|           $\mu_i$ = $\alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3$
|           $\alpha$ ~ Normal(178, 20)
|           $\beta_1$ ~ Log-Normal(0, 1)
|           $\beta_2$ ~ Normal(0, 1)
|           $\beta_3$ ~ Normal(0, 1)
|           $\sigma$ ~ Uniform(0, 50)

```{r}
d$weight_s3 <- d$weight_s^3
m4.6 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b1 * weight_s + b2 * weight_s2 + b3 * weight_s3,
    a ~ dnorm(178, 40),
    b1 ~ dlnorm(0, 2),
    b2 ~ dnorm(0, 20),
    b3 ~ dnorm(0, 20),
    sigma ~ dunif(0, 100)
  ), 
  data = d,
  )

weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight_s = weight.seq, 
                 weight_s2 = weight.seq^2,
                 weight_s3 = weight.seq^3)
mu <- link(m4.6, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.5, data = pred_dat)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)

plot(height ~ weight_s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

There are two clear problems:

  * a better fit to the sample might not actually be a better model
  * the model contains no biological information, so we can't learn about any causal relationship between height and weight
  
Linear models are useful because they are

  * easier to fit to data
  * easier to interpret, since they assume parameters act independently on the mean

Linear models are

  * too conventional
  * often used thoughtlessly
  * with real knowledge of your study system it is often easier to do better
  
Linear models are geocentric engines, used to describe partial correlations among variables. We should feel embarrased to use them.

Suppose you fit a model using standardized x-axis values, but want to plot estimates on the original scale? You can:

  * turn off the horizontal axis when you plot the data, using xaxt
  * explicitly construct the axis using the axis function
  
```{r}
plot(height ~ weight_s, d, col = col.alpha(rangi2, 0.5), xaxt = "n")
at <- c(-2, -1, 0, 1, 2)
labels <- at * sd(d$weight) + mean(d$weight)
axis(side = 1, at = at, labels = round(labels, 1))
```

### 4.5.2 Splines
