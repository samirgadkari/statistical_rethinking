---
title: "Ch5_the_many_variables_and_the_spurious_waffles"
author: "Samir Gadkari"
date: "4/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rethinking)
library(magrittr)
library(dagitty)
# library(DiagrammeR)
```

Taking the suggestions given during the attaching of the rethinking package, we call the requested functions:
```{r}
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

We will look at divorce rate and how it is related to:

  * marraige rate
  * median marraige age
  
for states within US. In this chapter, we will start thinking about:

  * spurious correlations between variables
  * revealing important correlations, masked by unrevealed correlations with other variables
  * start thinking about causal inference

** 5.1 Spurious association

```{r}
data(WaffleDivorce)
d <- WaffleDivorce

plot(d$Marriage, d$Divorce, xlab = "Marriage rate", ylab = "Divorce rate")
plot(d$MedianAgeMarriage, d$Divorce, 
     xlab = "Median age marriage", ylab = "Divorce rate")
```

High marriage rate implies high cultural valuation of marriage. Why would this cause the divorce rate to rise? Let's standardize the variables, and build models for both plots to investigate.

```{r}
d$A <- scale(d$MedianAgeMarriage)
d$D <- scale(d$Divorce)
```

Let's build a linear model like this:

|           $D_i$ ~ Normal($\mu_i$, $\sigma$)
|           $\mu_i$ = $\alpha + \beta_A A_i$
|           $\alpha$ ~ Normal(0, 0.2)
|           $\beta_A$ ~ Normal(0, 0.5)   This prior says 95% possible that
|                                             $\beta_A$ is between $\pm1$
|           $\sigma$ ~ Exponential(1)

What does the prior slope $\beta_A$ imply? If $\beta_A$ is 1, that means 1 sd change in Age at marriage is associated with a 1 sd change in Divorce rate. How big is that?

```{r}
sd(d$MedianAgeMarriage)
```

So a change of 1.2 years in the Age at marriage is associated with a full 1 sd change in Divorce rate. That seems like an insanely strong relationship. The above prior thinks only 5% of the plausible slopes are more extreme than 1.

```{r}
m5.1 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bA * A,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d)

set.seed(10)
prior <- extract.prior(m5.1)
mu <- link(m5.1, post = prior, data = list(A=c(-2, 2)))
plot(NULL, xlim = c(-2, 2), ylim = c(-2, 2))
for (i in 1:50)
  lines(c(-2, 2), mu[i,], col = col.alpha("black", 0.4))
```

Now for the posterior predictions - link and summarize with mean and PI, then plot.
```{r}
A_seq <- seq(from = -3, to = 3.2, length.out = 30)
mu <- link(m5.1, data = list(A = A_seq))
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(D ~ A, data = d, col = rangi2)
lines(A_seq, mu.mean, lwd = 2)
shade(mu.PI, A_seq)
```

Now for the relation between Marraige rate and Divorce rate:

```{r}
d$M <- scale(d$Marriage)
m5.2 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d)
```

If we go through and find the mean of bM, we can compare it with mean of bA. This is not a good way of deciding which predictor is better. Both could provide independent value, or one could eliminate the effects of the other. We have to think causally to understand what is going on.

```{r}
dag5.1 <- dagitty("dag {
                  A -> D
                  A -> M
                  M -> D
                  }")
coordinates(dag5.1) <- list(x = c(A = 0, D = 1, M = 2),
                            y = c(A = 0, D = 1, M = 0))
drawdag(dag5.1)
```

```{r include=TRUE, echo=FALSE}
# Use dagitty package instead of DiagrammeR, since that is what the
# code in the book uses.
# create_graph() %>%
#  add_node(label = "A") %>%
#  add_node(label = "M") %>%
#  add_node(label = "D") %>%
#  set_node_position(node = 1, x = 1, y = 2) %>%
#  set_node_position(node = 2, x = 3, y = 2) %>%
#  set_node_position(node = 3, x = 2, y = 1) %>%
#  add_edge(from = 1, to = 2) %>%
#  add_edge(from = 2, to = 3) %>%
#  add_edge(from = 1, to = 3) %>%
#  render_graph()
```

m5.1 tells us the _total_ influence of Age at marriage is strongly negative with divorce rate. The _total_ means we have to account for all paths from A to D.

m5.2 tells us that marraige rate is positively associated with divorce rate. It could be that the association between M and D arises entirely due to A's influence on both M and D:

```{r}
dag5.2 <- dagitty("dag {
                  A -> D
                  A -> M
                  }")
coordinates(dag5.2) <- list(x = c(A = 0, D = 1, M = 2),
                            y = c(A = 0, D = 1, M = 0))
drawdag(dag5.2)
```

```{r include=TRUE, echo=FALSE}
# Use dagitty package instead of DiagrammeR, since that is what the
# code in the book uses.
# create_graph() %>%
#  add_node(label = "A") %>%
#  add_node(label = "M") %>%
#  add_node(label = "D") %>%
# set_node_position(node = 1, x = 1, y = 2) %>%
#  set_node_position(node = 2, x = 3, y = 2) %>%
# set_node_position(node = 3, x = 2, y = 1) %>%
#  add_edge(from = 1, to = 2) %>%
#  add_edge(from = 1, to = 3) %>%
#  render_graph()
```


This DAG is also consistent with the posterior distributions of m5.1 and m5.2. So which is it? We need to slow down and carefully consider what each DAG implies.

### 5.1.2 Testable implications

A DAG may imply some variables are independent of others under certain conditions. These are the model's testable implications - it's conditional independencies. Conditional independencies tell us:

  * which variables should/should not be associated with one another in the data
  * which variables become disassociated when we condition on some other set of variables
  
Y $\not \perp$ X | Z  says that Y is independent of X after conditioning on Z.

For the first DAG, every variable is associated with the other variables. This is a testable assumption:

|               $D \not\perp A, D \not\perp M, A \not\perp M$

The $\not \perp$ symbol indicates "not independent of".

Sometimes, people use correlations (using the cor function), to determine which variables are associated with each other. This is terrible. Many different patterns of association with different implications can produce the same correlation.

In the second DAG, suppose we condition on A. M tells us nothing more about D, since information relevant to predicting D is already in A. So this is a testable assumption:

|              $D \perp M | A$

This same assumption cannot be used for the first DAG, because M really influences D in that model.

To see the implied conditional dependencies on the second DAG:

```{r}
DMA_dag2 <- dagitty('dag{ D <- A -> M }')
impliedConditionalIndependencies(DMA_dag2)
```

To see the implied conditional dependencies on the first DAG:

```{r}
DMA_dag1 <- dagitty('dag{ D <- A -> M -> D }')
impliedConditionalIndependencies(DMA_dag1)
```

For the first DAG, there are no implied conditional dependencies. So the only implication that differs between these DAGs is the last one: D _||_ M|A. To test this implication, we need a statistical model that conditions on A, so we can see if that renders D independent of M. That is what multiple regression helps with. It addresses the _descriptive_ question:

> Is there any additional value in knowing a variable, once I already know all of the other predictor variables?

In this case, the multiple-regression model addresses the questions:

  * After I know marriage rate, what additional value is there in also knowing age at marriage?
  * After I already know age at marriage, what additional value is there in also knowing marriage rate?
  
### 5.1.4 Approximating the posterior

|           $D_i$ ~ Normal($\mu_i$, $\sigma$)
|           $\mu_i$ = $\alpha + \beta_M M_i + \beta_A A_i$
|           $\alpha$ ~ Normal(0, 0.2)
|           $\beta_M$ ~ Normal(0, 0.5)
|           $\beta_A$ ~ Normal(0, 0.5)
|           $\sigma$ ~ Exponential(1)

```{r}
m5.3 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM*M + bA*A,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d)

precis(m5.3)
plot(coeftab(m5.1, m5.2, m5.3), par = c("bA", "bM"))
```

This plot shows the posterior means as circles, and the 89% compatibility interval as horizontal bars. It shows that although bM is high in m5.2, it's absolute value is much lower in m5.3. This means that once we know age at marraige, there is not much additional predictive power in also knowing the rate of marriage for that state.

This shows the implication of the second DAG D _||_ M | A. This means, the first DAG did not imply this result, so it is out. The association between marriage rate and divorce rate is spurious, caued by the influence of age at marriage on both marriage rate and divorce rate.

Simulating the divorce example M <- A -> D:
n <- 50
age <- rnorm(N)
div <- rnorm(N, -age)
mar <- rnorm(N, age)

Using these variables in m5.1, m5.2, m5.3 will give you the same pattern of posterior inferences as the actual data. To simulate influence of both A and M on D:

div <- rnorm(N, -age + mar)

Again, multiple regression will help sort things out.

Interpreting the parameter estimates depends on your belief about the causal model, because several causal models are consistent with any one set of parameter estimates.

### 5.1.5 Plotting multivariate posteriors

To find the relationship between age and divorce rate, we will:

  * standardize all variables (including the outcome variable)
  * create a model that predicts marriage rate based only on age
  * get the marriage rate residuals for the given age values
  * regress divorce rate on the marriage rate residuals
  
This will show us if there is a relationship between marriage rate and divorce rate, while taking into account the relationship between age and divorce rate. Similarly, we will find the relationship between marriage rate and divorce rate.

```{r}
m5.4 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + bA * A,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d)

mu <- link(m5.4)
mu_mean <- apply(mu, 2, mean)
mu_resid <- d$M - mu_mean

plot(mu_resid, d$D, col = rangi2,
     xlab = "Marriage rate residuals", ylab = "Divorce rate (std)")
```

You can see that there is no association between the marriage rate residuals, and the divorce rate, after taking into account all other variables. This means that there will also be no association between the marriage rate and the divorce rate.

The unified multivariate models do this automatically. It is useful to keep this fact in mind, because regressions behave in surprising ways as a result.

There is a tradition of taking residuals from one model and using them as data in another. _Never_ use residuals as data. They are parameters - variables with unobserved data. Treating them as known values throws away uncertainity. The right way to control a variable is to include it as a parameter in a model designed with an explicit causal identification strategy.

#### 5.1.5.2 Posterior prediction plots

Posterior prediction plots are used for:

  * check if the model correctly approximates the posterior
  * check where the model fails. This can help you improve the model, but it needs the analysts domain expertise.

Let's look at the posterior prediction plots for the unified model:

```{r}
mu <- link(m5.3) # when you don't specify data in link, it uses original data
mu_mean <- apply(mu, 2, mean)
mu_PI <- apply(mu, 2, PI)

D_sim <- sim(m5.3, n=1e3)
D_PI <- apply(D_sim, 2, PI)

plot(mu_mean ~ d$D, col = rangi2, ylim = range(mu_PI),
     xlab = "Observed divorce", ylab = "Predicted divorce")
abline(a = 0, b = 1, lty = 2)
for (i in 1:nrow(d))
  lines(rep(d$D[i], 2), mu_PI[, i], col = rangi2)

# If this line is run in a script, R will wait for you to click a point
# in the active plot window. It will place a label near that point,
# on the side you choose. When you are done labeling points,
# press the right mouse button, or ESC to exit this state.
# In the Rmarkdown, it does not do anything
# identify(x = d$D, y = mu_mean, labels = d$Loc)
```

A statistical model merely quantifies uncertainity based on the given model. It cannot answer if an effect is real or spurious. This depends on how well the modeller view of the world matches reality. For example, a spurious predictor could be eliminated by adding the correct predictor to the model. But, if you don't add such a predictor, you will never know the old predictor is spurious. Therefore, all statistical models are vulnerable and demand critique.

One way spurious associations between predictor and outcome can arise is when a truly causal predictor, x_real influences both the outcome and a spurious predictor x_spur. Let's simulate this:

```{r}
N <- 100
x_real <- rnorm(N)
x_spur <- rnorm(N, x_real)
y <- rnorm(N, x_real)
d <- data.frame(x_real, x_spur, y)

pairs(d)
```

You can see both x_spur and x_real are correlated with y. But when you include both in a linear regression predicting y, the posterior mean for the association between y and x_spur will be close to 0, while comparable mean for x_real will be closer to 1.

#### 5.1.5.3 Counterfactual plots

